# -*- coding: utf-8 -*-
"""cosine_similarity.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1T2eaZTxqcBVKPTvLvUz76rzWJqZo3L6f
"""

import heapq
import re
import string

import nltk
import spacy
from nltk.cluster.util import cosine_distance
import numpy as np
import networkx as nx

nltk.download('punkt')

nltk.download("stopwords")
!python -m spacy download pt_core_news_sm -q

#carregando arquivos txt

def load_txt(arq):
  with open(arq, "r") as file:
    texto = file.read()
  return texto

texto = load_txt("porquinhos.txt")

#Funcao que formata o texto. Retira stopwords, lematiza e passa tudo para minusculo
def format_lemma(txt):
  pln = spacy.load("pt_core_news_sm")
  txt = re.sub (r' +', ' ', txt)
  txt = txt.replace("\n\n", " ")
  txt = txt.lower()
  documento = pln(txt)
  tokens = []
  vicios = ['né', 'Então', 'então', 'gente', 'Né']
  stopwords = nltk.corpus.stopwords.words("portuguese") + vicios
  for i in documento:
    if i.text not in stopwords and i.text not in string.punctuation:
      tokens.append(i.lemma_)
  texto_formatado = " ".join(elemento for elemento in tokens if not elemento.isdigit())
  return texto_formatado

sentencas_originais = [sentenca for sentenca in nltk.sent_tokenize(texto)]
sentencas_formatadas = [format_lemma(sentenca_original) for sentenca_original in sentencas_originais]

def calcula_similaridade(sent1, sent2):
  palavras1 = [palavra for palavra in nltk.word_tokenize(sent1)]
  palavras2 = [palavra for palavra in nltk.word_tokenize(sent2)]

  todas_palavras = list(set(palavras1 + palavras2))

  vetor1 = [0]*len(todas_palavras)
  vetor2 = [0]*len(todas_palavras)

  for palavra in palavras1:
    vetor1[todas_palavras.index(palavra)] += 1
  for palavra in palavras2:
    vetor2[todas_palavras.index(palavra)] += 1
  
  return 1-cosine_distance(vetor1, vetor2)

def matriz_similaridade(sentencas):
  matriz_similar = np.zeros((len(sentencas), len(sentencas)))
  
  for i in range(len(sentencas)):
    for j in range(len(sentencas)):
      if i == j:
        continue
      matriz_similar[i][j] = calcula_similaridade(sentencas[i], sentencas[j])
  return matriz_similar

def sumarizar(texto,quant_sent):
  sentencas_originais = [sentenca for sentenca in nltk.sent_tokenize(texto)]
  sentencas_formatadas = [format_lemma(sentenca_original) for sentenca_original in sentencas_originais]
  
  matriz_de_similaridade = matriz_similaridade(sentencas_formatadas)

  grafo_similaridade = nx.from_numpy_array(matriz_de_similaridade)
  notas = nx.pagerank(grafo_similaridade, max_iter=1000000, tol= 1.0e-1)

  notas_ordenadas = sorted(((notas[i], nota) for i,nota in enumerate(sentencas_originais)), reverse=True)

  melhores_sentencas = []
  for i in range(quant_sent):
    melhores_sentencas.append(notas_ordenadas[i][1])



  resumo = ""
  for sentenca in sentencas_originais:
    if sentenca in melhores_sentencas:
      resumo += sentenca

  return sentencas_originais, melhores_sentencas,resumo

"""# Texto da internet"""

!pip install goose3 -q

from goose3 import Goose
from IPython.core.display import HTML

g = Goose()
url = "your_url"
artigo_internet = g.extract(url)
txt = artigo_internet.cleaned_text

def visualiza_sumarizacao(lista_sentencas, melhores_sentencas):
  texto = ""
  display(HTML(f'<h1>Resumo do texto</h1>'))
  for sentenca in lista_sentencas:
    if sentenca in melhores_sentencas:
      texto += sentenca.replace(sentenca, f'<mark>{sentenca}</mark>')
    else:
      texto += sentenca 
  return display(HTML(f"""{texto}"""))